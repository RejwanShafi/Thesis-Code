{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "MEL: 1113.0 cases\n",
      "NV: 6705.0 cases\n",
      "BCC: 514.0 cases\n",
      "AKIEC: 327.0 cases\n",
      "BKL: 1099.0 cases\n",
      "DF: 115.0 cases\n",
      "VASC: 142.0 cases\n",
      "Starting dataset augmentation...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid shape for coordinate array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 199\u001b[0m\n\u001b[0;32m    196\u001b[0m loader\u001b[38;5;241m.\u001b[39mget_class_distribution()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Perform augmentation\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 146\u001b[0m, in \u001b[0;36mSkinLesionDataLoader.augment_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m augment_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment_factors[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment_factors \u001b[38;5;28;01mif\u001b[39;00m row[col] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m aug_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(augment_factor):\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Apply augmentation\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     augmented_image, augmented_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# Save augmented image and mask\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     aug_image_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_aug_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maug_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[5], line 115\u001b[0m, in \u001b[0;36mSkinLesionDataLoader.augment\u001b[1;34m(self, image, mask)\u001b[0m\n\u001b[0;32m    112\u001b[0m augmented_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mwhere(augmented_mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Apply elastic deformation\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m augmented_image, augmented_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melastic_deformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmented_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m augmented_image, augmented_mask\n",
      "Cell \u001b[1;32mIn[5], line 90\u001b[0m, in \u001b[0;36mSkinLesionDataLoader.elastic_deformation\u001b[1;34m(self, image, mask, alpha, sigma)\u001b[0m\n\u001b[0;32m     87\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(y \u001b[38;5;241m+\u001b[39m dy, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), np\u001b[38;5;241m.\u001b[39mreshape(x \u001b[38;5;241m+\u001b[39m dx, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Apply deformation to the image and mask\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m deformed_image \u001b[38;5;241m=\u001b[39m \u001b[43mmap_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     91\u001b[0m deformed_mask \u001b[38;5;241m=\u001b[39m map_coordinates(mask, indices, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m deformed_image, deformed_mask\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\ndimage\\_interpolation.py:453\u001b[0m, in \u001b[0;36mmap_coordinates\u001b[1;34m(input, coordinates, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput and output rank must be > 0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coordinates\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid shape for coordinate array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    454\u001b[0m complex_output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39miscomplexobj(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    455\u001b[0m output \u001b[38;5;241m=\u001b[39m _ni_support\u001b[38;5;241m.\u001b[39m_get_output(output, \u001b[38;5;28minput\u001b[39m, shape\u001b[38;5;241m=\u001b[39moutput_shape,\n\u001b[0;32m    456\u001b[0m                                  complex_output\u001b[38;5;241m=\u001b[39mcomplex_output)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid shape for coordinate array"
     ]
    }
   ],
   "source": [
    "def preprocess_image(self, image_path): \"\"\" Load and preprocess an image. \"\"\" image = load_img(image_path, target_size=self.image_size) image = img_to_array(image) / 255.0 # Normalize to [0,1] return image def preprocess_mask(self, mask_path): \"\"\" Load and preprocess a mask, ensuring it's binary. \"\"\" mask = load_img(mask_path, target_size=self.image_size, color_mode='grayscale') mask = img_to_array(mask) / 255.0 # Normalize to [0,1] return np.where(mask > 0.5, 1, 0) # Binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\t2420322\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\t2420322\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\t2420322\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.5 MB 975.2 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.2/11.5 MB 1.7 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.5/11.5 MB 2.8 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.9/11.5 MB 4.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/11.5 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.5/11.5 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.7/11.5 MB 19.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.5 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 59.4 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/60.8 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------- ----- 51.2/60.8 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.8/60.8 kB 543.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in c:\\users\\t2420322\\appdata\\roaming\\python\\python312\\site-packages (from scipy) (2.0.2)\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/44.5 MB 3.2 MB/s eta 0:00:14\n",
      "   ---------------------------------------- 0.2/44.5 MB 2.6 MB/s eta 0:00:18\n",
      "   ---------------------------------------- 0.4/44.5 MB 3.4 MB/s eta 0:00:14\n",
      "    --------------------------------------- 1.0/44.5 MB 5.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 1.8/44.5 MB 8.3 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.8/44.5 MB 11.2 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 2.8/44.5 MB 11.2 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 2.8/44.5 MB 11.2 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 3.1/44.5 MB 7.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 6.2/44.5 MB 13.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 10.3/44.5 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.2/44.5 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 20.6/44.5 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.0/44.5 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.4/44.5 MB 131.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 36.7/44.5 MB 131.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 42.1/44.5 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.5/44.5 MB 129.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 81.8 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.image \n",
    "import load_img, img_to_array, save_img \n",
    "from scipy.ndimage import gaussian_filter, map_coordinates \n",
    "\n",
    "class SkinLesionDataLoader: def __init__(self, csv_path, image_dir, mask_dir, output_img_dir, output_mask_dir, augmented_csv_path, image_size=(128, 128), batch_size=32, seed=42): \n",
    "    \"\"\" Initialize the DataLoader with dataset paths and parameters. \n",
    "    Args: csv_path: Path to the CSV file with labels and metadata. \n",
    "    image_dir: Directory containing original images.\n",
    "    mask_dir: Directory containing mask images.\n",
    "    output_img_dir: Directory to save augmented images. \n",
    "    output_mask_dir: Directory to save augmented masks. \n",
    "    augmented_csv_path: Path to save the augmented dataset CSV. \n",
    "    image_size: Tuple of (height, width) for resizing images. \n",
    "    batch_size: Number of samples per batch. seed: Seed for reproducibility of augmentations. \"\"\" \n",
    "    self.csv_path = csv_path \n",
    "    self.image_dir = image_dir \n",
    "    self.mask_dir = mask_dir \n",
    "    self.output_img_dir = output_img_dir \n",
    "    self.output_mask_dir = output_mask_dir \n",
    "    self.augmented_csv_path = augmented_csv_path\n",
    "     self.image_size = image_size \n",
    "    self.batch_size = batch_size \n",
    "    self.seed = seed \n",
    "    # Read the original CSV \n",
    "    self.df = pd.read_csv(csv_path) \n",
    "    # Define augmentation factors \n",
    "    self.augment_factors = { 'MEL': 4, \n",
    "                            'NV': 1, # Skip augmentation for NV \n",
    "                            'BCC': 4, \n",
    "                            'AKIEC': 9, \n",
    "                            'BKL': 7, \n",
    "                            'DF': 18, \n",
    "                            'VASC': 15 } # Create output directories \n",
    "    os.makedirs(self.output_img_dir, exist_ok=True) \n",
    "    os.makedirs(self.output_mask_dir, exist_ok=True) \n",
    "    \n",
    "def preprocess_image(self, image_path):\n",
    "    \"\"\" Load and preprocess an image. \"\"\" \n",
    "    image = load_img(image_path, target_size=self.image_size) \n",
    "    image = img_to_array(image) / 255.0 # Normalize to [0,1] \n",
    "    return image \n",
    "\n",
    "def preprocess_mask(self, mask_path): \n",
    "    \"\"\" Load and preprocess a mask, ensuring it's binary. \"\"\" \n",
    "    mask = load_img(mask_path, target_size=self.image_size, color_mode='grayscale') \n",
    "    mask = img_to_array(mask) / 255.0 \n",
    "    # Normalize to [0,1] \n",
    "    return np.where(mask > 0.5, 1, 0) # Binarize \n",
    "\n",
    "def elastic_deformation(self, image, mask, alpha=34, sigma=4): \n",
    "    \"\"\" Apply elastic deformation to an image and its corresponding mask. \n",
    "    Args: image: The input image as a NumPy array.\n",
    "    mask: The corresponding mask as a NumPy array. \n",
    "    alpha: Scaling factor for the intensity of deformation. \n",
    "    sigma: Standard deviation of the Gaussian filter. \"\"\" \n",
    "    random_state = np.random.RandomState(self.seed) # Generate random displacement fields \n",
    "    shape = image.shape[:2] \n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha \n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha \n",
    "    # Create meshgrid for pixel indices \n",
    "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0])) \n",
    "    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1))\n",
    "    # Apply deformation to the image and mask \n",
    "    deformed_image = map_coordinates(image, indices, order=1, mode='reflect').reshape(image.shape) \n",
    "    deformed_mask = map_coordinates(mask, indices, order=1, mode='reflect').reshape(mask.shape) \n",
    "    return deformed_image, deformed_mask\n",
    "\n",
    "def augment(self, image, mask): \n",
    "    \"\"\" Apply augmentations to an image and its corresponding mask. \"\"\" \n",
    "    tf.random.set_seed(self.seed) \n",
    "    augmentation_pipeline = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\", seed=self.seed), \n",
    "        tf.keras.layers.RandomRotation(0.15, seed=self.seed), \n",
    "        tf.keras.layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), seed=self.seed), \n",
    "        tf.keras.layers.RandomContrast(0.2, seed=self.seed) \n",
    "        ]) \n",
    "    # Apply augmentations to both image and mask \n",
    "    augmented_image = augmentation_pipeline(image) \n",
    "    augmented_mask = augmentation_pipeline(mask) \n",
    "    # Ensure mask is binary \n",
    "    augmented_mask = tf.where(augmented_mask > 0.5, 1.0, 0.0) \n",
    "    # Apply CLAHE for contrast enhancement \n",
    "    augmented_image_uint8 = tf.cast(augmented_image * 255, tf.uint8).numpy() \n",
    "    lab = cv2.cvtColor(augmented_image_uint8, cv2.COLOR_RGB2LAB) \n",
    "    lab_planes = cv2.split(lab) \n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)) \n",
    "    lab_planes[0] = clahe.apply(lab_planes[0]) \n",
    "    lab = cv2.merge(lab_planes) \n",
    "    augmented_image = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB) / 255.0 \n",
    "    # Add minimal noise \n",
    "    noise = tf.random.normal(shape=tf.shape(augmented_image), mean=0.0, stddev=0.02) \n",
    "    augmented_image = augmented_image + noise \n",
    "    augmented_image = tf.clip_by_value(augmented_image, 0.0, 1.0) \n",
    "    # Apply elastic deformation \n",
    "    augmented_image, augmented_mask = self.elastic_deformation(augmented_image, augmented_mask.numpy()) \n",
    "    return augmented_image, augmented_mask\n",
    "\n",
    "def augment_dataset(self): \n",
    "    \"\"\" Augment the dataset, save augmented images and masks, and generate a new CSV file. \"\"\" \n",
    "    print(\"Starting dataset augmentation...\") \n",
    "    augmented_data = [] # List to hold augmented dataset information \n",
    "    for idx, row in self.df.iterrows(): \n",
    "        image_id = row['image'] \n",
    "        image_path = os.path.join(self.image_dir, image_id + '.jpg') \n",
    "        mask_path = os.path.join(self.mask_dir, image_id + '_Segmentation.png') \n",
    "        # Check if image and mask exist \n",
    "        if not os.path.exists(image_path) or not os.path.exists(mask_path): \n",
    "            print(f\"Warning: Missing files for {image_id}\") \n",
    "            continue \n",
    "        # Load and preprocess \n",
    "        image = self.preprocess_image(image_path) \n",
    "        mask = self.preprocess_mask(mask_path) \n",
    "        # Determine the augmentation factor \n",
    "        augment_factor = max(self.augment_factors[col] for col in self.augment_factors if row[col] == 1) \n",
    "        for aug_idx in range(augment_factor): \n",
    "            # Apply augmentation \n",
    "            augmented_image, augmented_mask = self.augment( tf.expand_dims(image, 0), tf.expand_dims(mask, 0) ) \n",
    "            # Save augmented image and mask\n",
    "            aug_image_id = f\"{image_id}_aug_{aug_idx}\" \n",
    "            aug_image_path = os.path.join(self.output_img_dir, aug_image_id + '.jpg') \n",
    "            aug_mask_path = os.path.join(self.output_mask_dir, aug_image_id + '.png') \n",
    "            save_img(aug_image_path, augmented_image * 255.0) # Convert to uint8 save_img(aug_mask_path, \n",
    "            augmented_mask * 255.0, scale=False) \n",
    "            # Append to augmented data \n",
    "            labels = [row[col] for col in ['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']] \n",
    "            augmented_data.append([aug_image_id, aug_image_path, aug_mask_path] + labels) \n",
    "        # Create DataFrame for augmented data \n",
    "        augmented_df = pd.DataFrame(augmented_data, columns=['image', 'image_path', 'mask_path', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']) \n",
    "        # Save augmented dataset to CSV \n",
    "        augmented_df.to_csv(self.augmented_csv_path, index=False) \n",
    "        print(f\"Augmentation completed. Augmented dataset saved to {self.augmented_csv_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
